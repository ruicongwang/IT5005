{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip3jN1YZtNXH"
      },
      "source": [
        "# IT5005 Artificial Intelligence Term Assignment\n",
        "\n",
        "Fill your name and student number below:\n",
        "\n",
        "\n",
        "| Student Number: | Name:                   |\n",
        "|:----------------|:------------------------|\n",
        "| A0244297W       | Wang Ruicong              |\n",
        "\n",
        "\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "In this assignment we hope to achieve the following:\n",
        "\n",
        "    1. An understanding of the practical limitations of using dense networks in complex tasks\n",
        "    2. Hands-on experience in building a deep learning neural network to solve a relatively complex task.\n",
        "    \n",
        "As this lab is more challenging than the previous labs, please work in teams of two persons. Please use the respective categories in the LumiNUS Forum under the \"Labs\" Heading to find a partner within your own group.\n",
        "\n",
        "Each step may take a long time to run. You and your partner may want to work out how to do things simultaneously, but please do not miss out on any learning opportunities.\n",
        "\n",
        "\n",
        "## 2. Submission Instructions\n",
        "\n",
        "\n",
        "### 2.1 SUBMISSION INSTRUCTIONS\n",
        "\n",
        "Please rename this Jupyter notebook to your student ID (e.g. A1234567Y.ipynb), complete it and submit to Canvas by 12 pm, Sunday 23 April 2023.\n",
        "\n",
        "The folder will close shortly after 12 pm on 23 April, after which you will no longer be able to submit your assignment and you will get 0.\n",
        "\n",
        "\n",
        "## 3. Creating a Dense Network for CIFAR-10\n",
        "\n",
        "We will now begin building a neural network for the CIFAR-10 dataset. The CIFAR-10 dataset consists of 50,000 32x32x3 (32x32 pixels, RGB channels) training images and 10,000 testing images (also 32x32x3), divided into the following 10 categories:\n",
        "\n",
        "    1. Airplane\n",
        "    2. Automobile\n",
        "    3. Bird\n",
        "    4. Cat\n",
        "    5. Deer\n",
        "    6. Dog\n",
        "    7. Frog\n",
        "    8. Horse\n",
        "    9. Ship\n",
        "    10. Truck\n",
        "    \n",
        "In the first two parts of this lab we will create a classifier for the CIFAR-10 dataset.\n",
        "\n",
        "### 3.1 Loading the Dataset\n",
        "\n",
        "We begin firstly by creating a Dense neural network for CIFAR-10. The code below shows how we load the CIFAR-10 dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-VTN6p1tNXJ",
        "outputId": "3e9b322c-bbe9-4b98-9fc2-7e7163def16e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "def load_cifar10():\n",
        "    (train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
        "    train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
        "    test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
        "    train_x = train_x.astype('float32')\n",
        "    test_x = test_x.astype('float32')\n",
        "    train_x /= 255.0\n",
        "    test_x /= 255.0\n",
        "    ret_train_y = to_categorical(train_y,10)\n",
        "    ret_test_y = to_categorical(test_y, 10)\n",
        "    \n",
        "    return (train_x, ret_train_y), (test_x, ret_test_y)\n",
        "\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = load_cifar10()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDqLk_aPtNXK"
      },
      "source": [
        "----\n",
        "\n",
        "#### Question 1\n",
        "\n",
        "Explain what the following two  statements do, and where the number \"3072\" came from (2 MARKS):\n",
        "\n",
        "```\n",
        "  train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
        "  test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
        "```\n",
        "\n",
        "***ANSWER: These two statements shape each original three dimension training and test image of 32 * 32 * 3 to a one dimension vector of 3072 length , and the number 3072 comes from 32 * 32 * 3 (width * height * number of channels(RGB)) .**\n",
        "\n",
        "*FOR GRADER: _______ / 2*\n",
        "\n",
        "### 3.2 Building the MLP Classifier\n",
        "\n",
        "In the code box below, create a new fully connected (dense) multilayer perceptron classifier for the CIFAR-10 dataset. To begin with, create a network with one hidden layer of 1024 neurons, using the SGD optimizer. You should output the training and validation accuracy at every epoch, and train for 50 epochs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoXCtRErtNXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bce7556-fe78-430a-f4c3-4fd187d5f102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 11s 4ms/step - loss: 1.8803 - accuracy: 0.3225 - val_loss: 1.7452 - val_accuracy: 0.3701\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7661 - accuracy: 0.3653 - val_loss: 1.6798 - val_accuracy: 0.3944\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.7013 - accuracy: 0.3892 - val_loss: 1.6690 - val_accuracy: 0.3976\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6660 - accuracy: 0.4023 - val_loss: 1.7008 - val_accuracy: 0.3753\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6233 - accuracy: 0.4184 - val_loss: 1.6324 - val_accuracy: 0.4078\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5993 - accuracy: 0.4277 - val_loss: 1.6836 - val_accuracy: 0.4075\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5737 - accuracy: 0.4384 - val_loss: 1.6360 - val_accuracy: 0.4271\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5467 - accuracy: 0.4497 - val_loss: 1.6438 - val_accuracy: 0.4107\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5209 - accuracy: 0.4588 - val_loss: 1.5859 - val_accuracy: 0.4371\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5179 - accuracy: 0.4618 - val_loss: 1.5930 - val_accuracy: 0.4441\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4885 - accuracy: 0.4742 - val_loss: 1.5560 - val_accuracy: 0.4494\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4694 - accuracy: 0.4772 - val_loss: 1.6032 - val_accuracy: 0.4325\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4583 - accuracy: 0.4835 - val_loss: 1.6488 - val_accuracy: 0.4257\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4326 - accuracy: 0.4937 - val_loss: 1.5443 - val_accuracy: 0.4629\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4277 - accuracy: 0.4951 - val_loss: 1.5742 - val_accuracy: 0.4505\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4007 - accuracy: 0.5035 - val_loss: 1.6276 - val_accuracy: 0.4537\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3929 - accuracy: 0.5059 - val_loss: 1.5633 - val_accuracy: 0.4544\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3759 - accuracy: 0.5151 - val_loss: 1.5545 - val_accuracy: 0.4637\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3662 - accuracy: 0.5181 - val_loss: 1.5592 - val_accuracy: 0.4668\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3533 - accuracy: 0.5194 - val_loss: 1.5332 - val_accuracy: 0.4728\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3242 - accuracy: 0.5299 - val_loss: 1.5377 - val_accuracy: 0.4814\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3222 - accuracy: 0.5358 - val_loss: 1.5635 - val_accuracy: 0.4678\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3003 - accuracy: 0.5382 - val_loss: 1.5402 - val_accuracy: 0.4695\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2938 - accuracy: 0.5404 - val_loss: 1.5721 - val_accuracy: 0.4696\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2795 - accuracy: 0.5462 - val_loss: 1.6000 - val_accuracy: 0.4703\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2667 - accuracy: 0.5506 - val_loss: 1.5914 - val_accuracy: 0.4586\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2532 - accuracy: 0.5571 - val_loss: 1.5199 - val_accuracy: 0.4862\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2354 - accuracy: 0.5631 - val_loss: 1.5641 - val_accuracy: 0.4790\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2161 - accuracy: 0.5686 - val_loss: 1.6187 - val_accuracy: 0.4672\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2104 - accuracy: 0.5702 - val_loss: 1.7001 - val_accuracy: 0.4603\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1992 - accuracy: 0.5751 - val_loss: 1.5899 - val_accuracy: 0.4969\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1851 - accuracy: 0.5787 - val_loss: 1.5667 - val_accuracy: 0.4880\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1678 - accuracy: 0.5860 - val_loss: 1.6468 - val_accuracy: 0.4757\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1570 - accuracy: 0.5888 - val_loss: 1.6155 - val_accuracy: 0.4704\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1397 - accuracy: 0.5974 - val_loss: 1.6258 - val_accuracy: 0.4737\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1309 - accuracy: 0.5995 - val_loss: 1.6383 - val_accuracy: 0.4727\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1201 - accuracy: 0.6027 - val_loss: 1.6276 - val_accuracy: 0.4745\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1009 - accuracy: 0.6092 - val_loss: 1.6100 - val_accuracy: 0.4914\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0925 - accuracy: 0.6108 - val_loss: 1.7009 - val_accuracy: 0.4690\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0757 - accuracy: 0.6159 - val_loss: 1.6361 - val_accuracy: 0.4937\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0748 - accuracy: 0.6162 - val_loss: 1.6869 - val_accuracy: 0.4693\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0627 - accuracy: 0.6252 - val_loss: 1.7409 - val_accuracy: 0.4677\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0447 - accuracy: 0.6296 - val_loss: 1.6945 - val_accuracy: 0.4803\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0308 - accuracy: 0.6348 - val_loss: 1.6375 - val_accuracy: 0.5046\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0206 - accuracy: 0.6395 - val_loss: 1.6829 - val_accuracy: 0.4898\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0131 - accuracy: 0.6398 - val_loss: 1.6646 - val_accuracy: 0.4946\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9877 - accuracy: 0.6491 - val_loss: 1.7182 - val_accuracy: 0.4803\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9975 - accuracy: 0.6463 - val_loss: 1.7655 - val_accuracy: 0.4738\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9764 - accuracy: 0.6562 - val_loss: 1.7388 - val_accuracy: 0.4853\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9613 - accuracy: 0.6552 - val_loss: 1.7994 - val_accuracy: 0.4846\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9562e01610>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\" \n",
        "Write your code to build an MLP with one hidden layer of 1024 neurons,\n",
        "with an SGD optimizer. Train for 50 epochs, and output the training and\n",
        "validation accuracy at each epoch.\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape=(3072,), activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics='accuracy')\n",
        "\n",
        "model.fit(train_x, train_y, epochs=50, batch_size=32, validation_data=(test_x, test_y))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix33QdWOtNXL"
      },
      "source": [
        "#### Question 2\n",
        "\n",
        "Complete the following table on the design choices for your MLP \n",
        "(3 MARKS):\n",
        "\n",
        "| Hyperparameter       | What I used | Why?                  |\n",
        "|:---------------------|:------------|:----------------------|\n",
        "| Optimizer            | SGD         | Specified in question |\n",
        "| # of hidden layers   | 1           | Specified in question |\n",
        "| # of hidden neurons  | 1024        | Specified in question |\n",
        "| Hid layer activation |    ReLU         | It can avoid gradient vanishing and the computation is simple and fast                   |\n",
        "| # of output neurons  |     10        |   There are 10 different classes in the dataset                    |\n",
        "| Output activation    |     Sofrmax        |  It can make the outputs represent the probabilitiy in each class, where the outputs are larger than 0 and are sum to 1                     |\n",
        "| lr                   |     0.01        |  A widely used learning rate value at the beginning stage                     |\n",
        "| momentum             |      0.9       |   A commonly used value for SGD                    |\n",
        "| decay                |     0        |   No need to decay the learning rate at first stage                    |\n",
        "| loss                 |    Categorical Cross Entropy         |          A commonly used loss function for multiclass classification             | \n",
        "\n",
        "*For TA: ___ / 3* <br>\n",
        "*Code:  ____/ 5* <br>\n",
        "**TOTAL: ____ / 8** <br>\n",
        "\n",
        "#### Question 3:\n",
        "\n",
        "What was your final training accuracy? Validation accuracy? Is there overfitting / underfitting? Explain your answer (5 MARKS)\n",
        "\n",
        "***The final training accuracy is 0.6552 and the final validation accuracy is 0.4846. There is overfitting problem because the training accuracy is much larger than the validation accuracy.***\n",
        "\n",
        "*FOR GRADER: ______ / 5*\n",
        "\n",
        "### 3.3 Experimenting with the MLP\n",
        "\n",
        "Cut and paste your code from Section 3.2 to the box below (you may need to rename your MLP). Experiment with the number of hidden layers, the number of neurons in each hidden layer, the optimization algorithm, etc. See [Keras Optimizers](https://keras.io/optimizers) for the types of optimizers and their parameters. **Train for 100 epochs.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xnqlGImtNXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbe5fef-72ac-49f1-d8f4-9b673fca9f37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8372 - accuracy: 0.3359 - val_loss: 1.7162 - val_accuracy: 0.3821\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6651 - accuracy: 0.4010 - val_loss: 1.5769 - val_accuracy: 0.4387\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5826 - accuracy: 0.4344 - val_loss: 1.5628 - val_accuracy: 0.4371\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5288 - accuracy: 0.4521 - val_loss: 1.4913 - val_accuracy: 0.4636\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4850 - accuracy: 0.4687 - val_loss: 1.5451 - val_accuracy: 0.4395\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4452 - accuracy: 0.4823 - val_loss: 1.5657 - val_accuracy: 0.4351\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4073 - accuracy: 0.4942 - val_loss: 1.4578 - val_accuracy: 0.4737\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3717 - accuracy: 0.5070 - val_loss: 1.4394 - val_accuracy: 0.4885\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3453 - accuracy: 0.5173 - val_loss: 1.5101 - val_accuracy: 0.4621\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3171 - accuracy: 0.5250 - val_loss: 1.4405 - val_accuracy: 0.4936\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2826 - accuracy: 0.5405 - val_loss: 1.4377 - val_accuracy: 0.4884\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2531 - accuracy: 0.5493 - val_loss: 1.4551 - val_accuracy: 0.4870\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2289 - accuracy: 0.5584 - val_loss: 1.4549 - val_accuracy: 0.4938\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2012 - accuracy: 0.5685 - val_loss: 1.4074 - val_accuracy: 0.5033\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1763 - accuracy: 0.5762 - val_loss: 1.3953 - val_accuracy: 0.5139\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1434 - accuracy: 0.5889 - val_loss: 1.4388 - val_accuracy: 0.5085\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1229 - accuracy: 0.5975 - val_loss: 1.4853 - val_accuracy: 0.4989\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0929 - accuracy: 0.6056 - val_loss: 1.4594 - val_accuracy: 0.5162\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0624 - accuracy: 0.6183 - val_loss: 1.4575 - val_accuracy: 0.5102\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0448 - accuracy: 0.6232 - val_loss: 1.5193 - val_accuracy: 0.5020\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0193 - accuracy: 0.6332 - val_loss: 1.5268 - val_accuracy: 0.5034\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9993 - accuracy: 0.6416 - val_loss: 1.5194 - val_accuracy: 0.5066\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9731 - accuracy: 0.6515 - val_loss: 1.6183 - val_accuracy: 0.5113\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9527 - accuracy: 0.6575 - val_loss: 1.5776 - val_accuracy: 0.5101\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9280 - accuracy: 0.6676 - val_loss: 1.5720 - val_accuracy: 0.5120\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9032 - accuracy: 0.6756 - val_loss: 1.6548 - val_accuracy: 0.4962\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8846 - accuracy: 0.6810 - val_loss: 1.6515 - val_accuracy: 0.5228\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8543 - accuracy: 0.6947 - val_loss: 1.6379 - val_accuracy: 0.5069\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8468 - accuracy: 0.6961 - val_loss: 1.7315 - val_accuracy: 0.5075\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8063 - accuracy: 0.7112 - val_loss: 1.8193 - val_accuracy: 0.5014\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7946 - accuracy: 0.7175 - val_loss: 1.8108 - val_accuracy: 0.5043\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7750 - accuracy: 0.7233 - val_loss: 1.9278 - val_accuracy: 0.5091\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7630 - accuracy: 0.7278 - val_loss: 1.9098 - val_accuracy: 0.5175\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7550 - accuracy: 0.7301 - val_loss: 1.8743 - val_accuracy: 0.4975\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7387 - accuracy: 0.7375 - val_loss: 1.8274 - val_accuracy: 0.5126\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7075 - accuracy: 0.7481 - val_loss: 2.0776 - val_accuracy: 0.4925\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7083 - accuracy: 0.7496 - val_loss: 2.1601 - val_accuracy: 0.4928\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6924 - accuracy: 0.7541 - val_loss: 2.0121 - val_accuracy: 0.5104\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6689 - accuracy: 0.7629 - val_loss: 2.1012 - val_accuracy: 0.4935\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6672 - accuracy: 0.7651 - val_loss: 2.0475 - val_accuracy: 0.5022\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6409 - accuracy: 0.7705 - val_loss: 2.0611 - val_accuracy: 0.4932\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6185 - accuracy: 0.7797 - val_loss: 2.3638 - val_accuracy: 0.4839\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6344 - accuracy: 0.7769 - val_loss: 2.2291 - val_accuracy: 0.4938\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6163 - accuracy: 0.7847 - val_loss: 2.3098 - val_accuracy: 0.5021\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6004 - accuracy: 0.7895 - val_loss: 2.3949 - val_accuracy: 0.4928\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5851 - accuracy: 0.7942 - val_loss: 2.4688 - val_accuracy: 0.4863\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5727 - accuracy: 0.7998 - val_loss: 2.4369 - val_accuracy: 0.4879\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5802 - accuracy: 0.7985 - val_loss: 2.4635 - val_accuracy: 0.4979\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5819 - accuracy: 0.7987 - val_loss: 2.5827 - val_accuracy: 0.4950\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5595 - accuracy: 0.8057 - val_loss: 2.5138 - val_accuracy: 0.5008\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.5509 - accuracy: 0.8090 - val_loss: 2.3983 - val_accuracy: 0.4894\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5512 - accuracy: 0.8093 - val_loss: 2.6679 - val_accuracy: 0.5021\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5232 - accuracy: 0.8188 - val_loss: 2.5526 - val_accuracy: 0.4795\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5303 - accuracy: 0.8172 - val_loss: 2.6352 - val_accuracy: 0.4998\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5249 - accuracy: 0.8207 - val_loss: 2.9527 - val_accuracy: 0.5025\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5316 - accuracy: 0.8186 - val_loss: 2.6270 - val_accuracy: 0.4898\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5347 - accuracy: 0.8163 - val_loss: 2.7225 - val_accuracy: 0.4774\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.5071 - accuracy: 0.8261 - val_loss: 2.8096 - val_accuracy: 0.4927\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5111 - accuracy: 0.8264 - val_loss: 2.6104 - val_accuracy: 0.4808\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4975 - accuracy: 0.8305 - val_loss: 2.9780 - val_accuracy: 0.4999\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5300 - accuracy: 0.8215 - val_loss: 2.9056 - val_accuracy: 0.4851\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5297 - accuracy: 0.8228 - val_loss: 2.9803 - val_accuracy: 0.4823\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5033 - accuracy: 0.8294 - val_loss: 3.1967 - val_accuracy: 0.4990\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4985 - accuracy: 0.8323 - val_loss: 3.0416 - val_accuracy: 0.4972\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4525 - accuracy: 0.8470 - val_loss: 3.7369 - val_accuracy: 0.5040\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4587 - accuracy: 0.8464 - val_loss: 3.0101 - val_accuracy: 0.4853\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4654 - accuracy: 0.8429 - val_loss: 3.0118 - val_accuracy: 0.4760\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4995 - accuracy: 0.8356 - val_loss: 3.2245 - val_accuracy: 0.4792\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5012 - accuracy: 0.8331 - val_loss: 3.1398 - val_accuracy: 0.4729\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5004 - accuracy: 0.8363 - val_loss: 3.3487 - val_accuracy: 0.4895\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4696 - accuracy: 0.8455 - val_loss: 3.6663 - val_accuracy: 0.5080\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4638 - accuracy: 0.8488 - val_loss: 4.0036 - val_accuracy: 0.4872\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4740 - accuracy: 0.8441 - val_loss: 3.8345 - val_accuracy: 0.5037\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4647 - accuracy: 0.8472 - val_loss: 3.3912 - val_accuracy: 0.4931\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4438 - accuracy: 0.8542 - val_loss: 3.1448 - val_accuracy: 0.4845\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4510 - accuracy: 0.8548 - val_loss: 3.3553 - val_accuracy: 0.4799\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4746 - accuracy: 0.8467 - val_loss: 3.7279 - val_accuracy: 0.4905\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4948 - accuracy: 0.8429 - val_loss: 3.8988 - val_accuracy: 0.4876\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5086 - accuracy: 0.8406 - val_loss: 3.4847 - val_accuracy: 0.4871\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4958 - accuracy: 0.8417 - val_loss: 3.3303 - val_accuracy: 0.4866\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4470 - accuracy: 0.8575 - val_loss: 3.2965 - val_accuracy: 0.4674\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4669 - accuracy: 0.8509 - val_loss: 4.1120 - val_accuracy: 0.4938\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4581 - accuracy: 0.8543 - val_loss: 3.6543 - val_accuracy: 0.4915\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4385 - accuracy: 0.8611 - val_loss: 4.1662 - val_accuracy: 0.4981\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4752 - accuracy: 0.8528 - val_loss: 4.1945 - val_accuracy: 0.4917\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4329 - accuracy: 0.8616 - val_loss: 4.0202 - val_accuracy: 0.4921\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4474 - accuracy: 0.8584 - val_loss: 3.8576 - val_accuracy: 0.4829\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4461 - accuracy: 0.8604 - val_loss: 4.5059 - val_accuracy: 0.4994\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4345 - accuracy: 0.8630 - val_loss: 4.1686 - val_accuracy: 0.4889\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4602 - accuracy: 0.8597 - val_loss: 3.7259 - val_accuracy: 0.4775\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4642 - accuracy: 0.8562 - val_loss: 4.3590 - val_accuracy: 0.4963\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4284 - accuracy: 0.8669 - val_loss: 4.3872 - val_accuracy: 0.4939\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4511 - accuracy: 0.8605 - val_loss: 4.4120 - val_accuracy: 0.4930\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4737 - accuracy: 0.8556 - val_loss: 5.2311 - val_accuracy: 0.4929\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4590 - accuracy: 0.8594 - val_loss: 4.0462 - val_accuracy: 0.4750\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5204 - accuracy: 0.8445 - val_loss: 4.8339 - val_accuracy: 0.4923\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4331 - accuracy: 0.8658 - val_loss: 4.7348 - val_accuracy: 0.5038\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4732 - accuracy: 0.8557 - val_loss: 4.6580 - val_accuracy: 0.4874\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4886 - accuracy: 0.8557 - val_loss: 5.0876 - val_accuracy: 0.4950\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4673 - accuracy: 0.8612 - val_loss: 4.8463 - val_accuracy: 0.4938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f953c11e4f0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\"\"\"\n",
        "Cut and paste your code from Section 3.2 below, then modify it to get\n",
        "much better results than what you had earlier. E.g. increase the number of\n",
        "nodes in the hidden layer, increase the number of hidden layers,\n",
        "change the optimizer, etc. \n",
        "\n",
        "Train for 100 epochs.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(1024, input_shape=(3072,), activation='relu'))\n",
        "model1.add(Dense(1024, activation='relu'))\n",
        "model1.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy', optimizer=sgd, metrics='accuracy')\n",
        "\n",
        "model1.fit(train_x, train_y, epochs=100, batch_size=32, validation_data=(test_x, test_y))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow94WT_QD5Xv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541d0bcf-a394-4612-b851-26e132d4e265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8381 - accuracy: 0.3329 - val_loss: 1.7208 - val_accuracy: 0.3870\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6664 - accuracy: 0.4055 - val_loss: 1.6094 - val_accuracy: 0.4234\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5902 - accuracy: 0.4281 - val_loss: 1.5572 - val_accuracy: 0.4408\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5291 - accuracy: 0.4517 - val_loss: 1.5431 - val_accuracy: 0.4515\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4909 - accuracy: 0.4668 - val_loss: 1.5042 - val_accuracy: 0.4618\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4541 - accuracy: 0.4767 - val_loss: 1.5198 - val_accuracy: 0.4442\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4184 - accuracy: 0.4924 - val_loss: 1.5273 - val_accuracy: 0.4632\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3938 - accuracy: 0.5003 - val_loss: 1.4598 - val_accuracy: 0.4785\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3583 - accuracy: 0.5115 - val_loss: 1.4216 - val_accuracy: 0.4944\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3397 - accuracy: 0.5208 - val_loss: 1.4442 - val_accuracy: 0.4915\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3150 - accuracy: 0.5315 - val_loss: 1.4901 - val_accuracy: 0.4732\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2892 - accuracy: 0.5407 - val_loss: 1.4603 - val_accuracy: 0.4876\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2675 - accuracy: 0.5466 - val_loss: 1.4515 - val_accuracy: 0.4892\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2458 - accuracy: 0.5520 - val_loss: 1.4538 - val_accuracy: 0.4958\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2249 - accuracy: 0.5601 - val_loss: 1.4760 - val_accuracy: 0.4855\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2020 - accuracy: 0.5686 - val_loss: 1.4012 - val_accuracy: 0.5189\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1918 - accuracy: 0.5744 - val_loss: 1.4410 - val_accuracy: 0.5125\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1678 - accuracy: 0.5796 - val_loss: 1.4444 - val_accuracy: 0.5035\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1411 - accuracy: 0.5889 - val_loss: 1.4922 - val_accuracy: 0.4921\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1293 - accuracy: 0.5965 - val_loss: 1.5375 - val_accuracy: 0.4882\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1013 - accuracy: 0.6049 - val_loss: 1.4901 - val_accuracy: 0.4991\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0889 - accuracy: 0.6088 - val_loss: 1.4707 - val_accuracy: 0.5077\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0699 - accuracy: 0.6165 - val_loss: 1.5386 - val_accuracy: 0.5021\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0560 - accuracy: 0.6201 - val_loss: 1.4752 - val_accuracy: 0.5096\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0344 - accuracy: 0.6281 - val_loss: 1.4911 - val_accuracy: 0.5183\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0239 - accuracy: 0.6314 - val_loss: 1.5198 - val_accuracy: 0.4941\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0048 - accuracy: 0.6389 - val_loss: 1.5522 - val_accuracy: 0.5144\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9869 - accuracy: 0.6454 - val_loss: 1.5913 - val_accuracy: 0.5095\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9709 - accuracy: 0.6509 - val_loss: 1.6232 - val_accuracy: 0.4950\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9562 - accuracy: 0.6545 - val_loss: 1.6806 - val_accuracy: 0.4944\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9389 - accuracy: 0.6616 - val_loss: 1.6156 - val_accuracy: 0.4992\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9201 - accuracy: 0.6695 - val_loss: 1.6495 - val_accuracy: 0.5049\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9088 - accuracy: 0.6713 - val_loss: 1.6716 - val_accuracy: 0.5123\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8895 - accuracy: 0.6807 - val_loss: 1.7397 - val_accuracy: 0.4945\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8795 - accuracy: 0.6838 - val_loss: 1.6890 - val_accuracy: 0.4999\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8666 - accuracy: 0.6895 - val_loss: 1.6849 - val_accuracy: 0.5138\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8401 - accuracy: 0.6989 - val_loss: 1.7307 - val_accuracy: 0.5176\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8355 - accuracy: 0.7018 - val_loss: 1.7801 - val_accuracy: 0.5069\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8170 - accuracy: 0.7077 - val_loss: 1.8710 - val_accuracy: 0.5017\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8153 - accuracy: 0.7072 - val_loss: 1.9333 - val_accuracy: 0.5021\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7827 - accuracy: 0.7199 - val_loss: 1.7757 - val_accuracy: 0.4975\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7781 - accuracy: 0.7217 - val_loss: 1.8716 - val_accuracy: 0.4946\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7802 - accuracy: 0.7222 - val_loss: 2.0546 - val_accuracy: 0.4997\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7700 - accuracy: 0.7243 - val_loss: 1.9753 - val_accuracy: 0.5069\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7572 - accuracy: 0.7314 - val_loss: 1.9599 - val_accuracy: 0.5017\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7501 - accuracy: 0.7336 - val_loss: 1.9520 - val_accuracy: 0.4944\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7338 - accuracy: 0.7377 - val_loss: 2.1032 - val_accuracy: 0.5130\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7232 - accuracy: 0.7438 - val_loss: 2.0970 - val_accuracy: 0.4944\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7129 - accuracy: 0.7476 - val_loss: 2.2484 - val_accuracy: 0.5040\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6970 - accuracy: 0.7539 - val_loss: 2.1005 - val_accuracy: 0.4747\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6867 - accuracy: 0.7556 - val_loss: 2.2579 - val_accuracy: 0.5025\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6625 - accuracy: 0.7636 - val_loss: 2.1941 - val_accuracy: 0.5142\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6744 - accuracy: 0.7611 - val_loss: 2.1643 - val_accuracy: 0.5023\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6649 - accuracy: 0.7650 - val_loss: 2.3427 - val_accuracy: 0.5028\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6526 - accuracy: 0.7694 - val_loss: 2.2248 - val_accuracy: 0.4743\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6511 - accuracy: 0.7689 - val_loss: 2.3684 - val_accuracy: 0.4914\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6233 - accuracy: 0.7801 - val_loss: 2.3428 - val_accuracy: 0.4925\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6418 - accuracy: 0.7750 - val_loss: 2.4323 - val_accuracy: 0.4951\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6273 - accuracy: 0.7805 - val_loss: 2.5172 - val_accuracy: 0.4868\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6058 - accuracy: 0.7866 - val_loss: 2.4633 - val_accuracy: 0.5075\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6094 - accuracy: 0.7854 - val_loss: 2.5535 - val_accuracy: 0.4991\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6048 - accuracy: 0.7860 - val_loss: 2.5860 - val_accuracy: 0.5049\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6009 - accuracy: 0.7906 - val_loss: 2.4787 - val_accuracy: 0.4896\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5969 - accuracy: 0.7925 - val_loss: 2.6828 - val_accuracy: 0.4893\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5853 - accuracy: 0.7961 - val_loss: 2.6946 - val_accuracy: 0.4986\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5915 - accuracy: 0.7947 - val_loss: 2.4269 - val_accuracy: 0.4906\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5663 - accuracy: 0.8019 - val_loss: 2.6490 - val_accuracy: 0.4937\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5598 - accuracy: 0.8055 - val_loss: 2.5132 - val_accuracy: 0.4938\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5569 - accuracy: 0.8049 - val_loss: 2.7253 - val_accuracy: 0.5017\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5541 - accuracy: 0.8066 - val_loss: 2.7201 - val_accuracy: 0.4885\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5318 - accuracy: 0.8138 - val_loss: 2.8519 - val_accuracy: 0.5013\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5574 - accuracy: 0.8063 - val_loss: 2.8773 - val_accuracy: 0.4928\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5646 - accuracy: 0.8061 - val_loss: 2.7918 - val_accuracy: 0.4759\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5369 - accuracy: 0.8142 - val_loss: 2.8974 - val_accuracy: 0.4623\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5424 - accuracy: 0.8143 - val_loss: 2.8296 - val_accuracy: 0.4957\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5176 - accuracy: 0.8220 - val_loss: 3.0371 - val_accuracy: 0.4884\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5185 - accuracy: 0.8245 - val_loss: 3.0857 - val_accuracy: 0.5023\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5346 - accuracy: 0.8172 - val_loss: 3.1457 - val_accuracy: 0.4832\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5344 - accuracy: 0.8184 - val_loss: 3.0632 - val_accuracy: 0.4859\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5134 - accuracy: 0.8227 - val_loss: 3.1643 - val_accuracy: 0.4976\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5132 - accuracy: 0.8246 - val_loss: 3.2076 - val_accuracy: 0.5019\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5263 - accuracy: 0.8220 - val_loss: 2.9396 - val_accuracy: 0.4924\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5150 - accuracy: 0.8274 - val_loss: 3.1402 - val_accuracy: 0.4932\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5152 - accuracy: 0.8253 - val_loss: 2.9893 - val_accuracy: 0.4862\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4971 - accuracy: 0.8310 - val_loss: 3.4118 - val_accuracy: 0.5042\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4885 - accuracy: 0.8335 - val_loss: 3.3280 - val_accuracy: 0.4976\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5081 - accuracy: 0.8298 - val_loss: 3.2278 - val_accuracy: 0.4884\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4856 - accuracy: 0.8353 - val_loss: 3.2692 - val_accuracy: 0.4866\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5038 - accuracy: 0.8312 - val_loss: 3.5472 - val_accuracy: 0.4909\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4995 - accuracy: 0.8321 - val_loss: 3.6904 - val_accuracy: 0.4920\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4797 - accuracy: 0.8389 - val_loss: 3.5945 - val_accuracy: 0.4930\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4896 - accuracy: 0.8371 - val_loss: 3.5461 - val_accuracy: 0.4892\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4891 - accuracy: 0.8345 - val_loss: 3.4702 - val_accuracy: 0.5009\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4742 - accuracy: 0.8404 - val_loss: 3.6504 - val_accuracy: 0.4960\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5153 - accuracy: 0.8317 - val_loss: 3.6952 - val_accuracy: 0.4897\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5132 - accuracy: 0.8287 - val_loss: 4.1733 - val_accuracy: 0.5022\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4613 - accuracy: 0.8458 - val_loss: 3.8867 - val_accuracy: 0.5005\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4901 - accuracy: 0.8378 - val_loss: 3.7788 - val_accuracy: 0.4987\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5007 - accuracy: 0.8346 - val_loss: 3.6780 - val_accuracy: 0.4928\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4922 - accuracy: 0.8397 - val_loss: 3.7989 - val_accuracy: 0.4947\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f956c0902b0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(1024, input_shape=(3072,), activation='relu'))\n",
        "model2.add(Dense(512, activation='relu'))\n",
        "model2.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics='accuracy')\n",
        "\n",
        "model2.fit(train_x, train_y, epochs=100, batch_size=32, validation_data=(test_x, test_y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4quom6ljD5gQ",
        "outputId": "6bf25c3e-c983-4196-f491-12b6344b7605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8334 - accuracy: 0.3352 - val_loss: 1.7390 - val_accuracy: 0.3735\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6542 - accuracy: 0.4053 - val_loss: 1.5463 - val_accuracy: 0.4459\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5709 - accuracy: 0.4370 - val_loss: 1.5343 - val_accuracy: 0.4551\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5136 - accuracy: 0.4568 - val_loss: 1.5305 - val_accuracy: 0.4505\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4676 - accuracy: 0.4754 - val_loss: 1.4958 - val_accuracy: 0.4665\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4159 - accuracy: 0.4924 - val_loss: 1.4431 - val_accuracy: 0.4869\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3805 - accuracy: 0.5059 - val_loss: 1.4425 - val_accuracy: 0.4832\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3387 - accuracy: 0.5178 - val_loss: 1.4236 - val_accuracy: 0.4987\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2978 - accuracy: 0.5351 - val_loss: 1.4533 - val_accuracy: 0.4908\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2595 - accuracy: 0.5475 - val_loss: 1.4156 - val_accuracy: 0.5106\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2249 - accuracy: 0.5585 - val_loss: 1.4078 - val_accuracy: 0.5051\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1876 - accuracy: 0.5729 - val_loss: 1.4698 - val_accuracy: 0.5001\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1640 - accuracy: 0.5832 - val_loss: 1.4025 - val_accuracy: 0.5167\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1252 - accuracy: 0.5968 - val_loss: 1.4565 - val_accuracy: 0.5057\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.0870 - accuracy: 0.6086 - val_loss: 1.4609 - val_accuracy: 0.5038\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0527 - accuracy: 0.6204 - val_loss: 1.4800 - val_accuracy: 0.5061\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.0133 - accuracy: 0.6365 - val_loss: 1.4888 - val_accuracy: 0.5089\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9781 - accuracy: 0.6500 - val_loss: 1.4992 - val_accuracy: 0.5179\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9475 - accuracy: 0.6600 - val_loss: 1.5786 - val_accuracy: 0.5100\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9063 - accuracy: 0.6745 - val_loss: 1.5381 - val_accuracy: 0.5070\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8718 - accuracy: 0.6878 - val_loss: 1.6126 - val_accuracy: 0.5216\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8386 - accuracy: 0.6975 - val_loss: 1.5870 - val_accuracy: 0.5140\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7931 - accuracy: 0.7138 - val_loss: 1.6900 - val_accuracy: 0.5187\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7730 - accuracy: 0.7212 - val_loss: 1.6965 - val_accuracy: 0.5137\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7369 - accuracy: 0.7371 - val_loss: 1.8148 - val_accuracy: 0.5139\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7020 - accuracy: 0.7503 - val_loss: 1.7333 - val_accuracy: 0.5146\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6799 - accuracy: 0.7584 - val_loss: 1.7685 - val_accuracy: 0.5209\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6468 - accuracy: 0.7689 - val_loss: 1.8792 - val_accuracy: 0.5169\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6279 - accuracy: 0.7751 - val_loss: 1.9711 - val_accuracy: 0.5181\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6020 - accuracy: 0.7846 - val_loss: 1.8946 - val_accuracy: 0.5097\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5717 - accuracy: 0.7952 - val_loss: 1.9914 - val_accuracy: 0.5044\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.5607 - accuracy: 0.8022 - val_loss: 2.0153 - val_accuracy: 0.5147\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5373 - accuracy: 0.8099 - val_loss: 2.2265 - val_accuracy: 0.5045\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.5242 - accuracy: 0.8140 - val_loss: 2.0651 - val_accuracy: 0.5109\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4932 - accuracy: 0.8252 - val_loss: 2.4428 - val_accuracy: 0.4945\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4810 - accuracy: 0.8314 - val_loss: 2.2944 - val_accuracy: 0.5234\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4631 - accuracy: 0.8366 - val_loss: 2.2561 - val_accuracy: 0.5075\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4461 - accuracy: 0.8443 - val_loss: 2.5548 - val_accuracy: 0.5041\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4444 - accuracy: 0.8441 - val_loss: 2.4624 - val_accuracy: 0.5084\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4442 - accuracy: 0.8461 - val_loss: 2.5149 - val_accuracy: 0.4984\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4359 - accuracy: 0.8487 - val_loss: 2.4653 - val_accuracy: 0.5169\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4301 - accuracy: 0.8525 - val_loss: 2.6798 - val_accuracy: 0.5107\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4003 - accuracy: 0.8631 - val_loss: 2.7213 - val_accuracy: 0.5024\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3974 - accuracy: 0.8625 - val_loss: 2.8886 - val_accuracy: 0.5008\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4029 - accuracy: 0.8617 - val_loss: 2.9049 - val_accuracy: 0.5123\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3848 - accuracy: 0.8691 - val_loss: 2.7854 - val_accuracy: 0.5075\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3537 - accuracy: 0.8793 - val_loss: 3.0343 - val_accuracy: 0.4945\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3783 - accuracy: 0.8723 - val_loss: 3.1715 - val_accuracy: 0.5077\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3743 - accuracy: 0.8741 - val_loss: 2.9463 - val_accuracy: 0.5058\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3595 - accuracy: 0.8786 - val_loss: 3.5386 - val_accuracy: 0.5096\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3262 - accuracy: 0.8880 - val_loss: 3.0128 - val_accuracy: 0.5086\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3868 - accuracy: 0.8700 - val_loss: 3.0596 - val_accuracy: 0.4964\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3506 - accuracy: 0.8835 - val_loss: 3.3670 - val_accuracy: 0.5084\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3269 - accuracy: 0.8912 - val_loss: 3.2647 - val_accuracy: 0.5154\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3390 - accuracy: 0.8887 - val_loss: 3.3746 - val_accuracy: 0.5126\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3356 - accuracy: 0.8896 - val_loss: 3.6199 - val_accuracy: 0.5176\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3549 - accuracy: 0.8845 - val_loss: 3.3837 - val_accuracy: 0.5057\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3618 - accuracy: 0.8830 - val_loss: 3.7804 - val_accuracy: 0.5168\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3022 - accuracy: 0.9013 - val_loss: 3.4717 - val_accuracy: 0.5034\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3325 - accuracy: 0.8899 - val_loss: 3.8237 - val_accuracy: 0.5138\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3019 - accuracy: 0.9024 - val_loss: 3.9209 - val_accuracy: 0.5185\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3256 - accuracy: 0.8962 - val_loss: 3.9182 - val_accuracy: 0.5078\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3373 - accuracy: 0.8932 - val_loss: 3.6783 - val_accuracy: 0.5000\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3637 - accuracy: 0.8859 - val_loss: 3.9905 - val_accuracy: 0.5072\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3657 - accuracy: 0.8852 - val_loss: 3.8071 - val_accuracy: 0.5120\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3422 - accuracy: 0.8916 - val_loss: 3.7663 - val_accuracy: 0.5185\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.2858 - accuracy: 0.9077 - val_loss: 3.8106 - val_accuracy: 0.5154\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3200 - accuracy: 0.9005 - val_loss: 3.7872 - val_accuracy: 0.5023\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4002 - accuracy: 0.8788 - val_loss: 4.3371 - val_accuracy: 0.5083\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3364 - accuracy: 0.8951 - val_loss: 4.1046 - val_accuracy: 0.5050\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3293 - accuracy: 0.8980 - val_loss: 4.0986 - val_accuracy: 0.5023\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3272 - accuracy: 0.9000 - val_loss: 4.4521 - val_accuracy: 0.5043\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3334 - accuracy: 0.8990 - val_loss: 4.4765 - val_accuracy: 0.5201\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3364 - accuracy: 0.8984 - val_loss: 4.6770 - val_accuracy: 0.5176\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3387 - accuracy: 0.8979 - val_loss: 4.8862 - val_accuracy: 0.4951\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3245 - accuracy: 0.9004 - val_loss: 4.7428 - val_accuracy: 0.5076\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3337 - accuracy: 0.9000 - val_loss: 4.7347 - val_accuracy: 0.5083\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3925 - accuracy: 0.8872 - val_loss: 4.6460 - val_accuracy: 0.5104\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3301 - accuracy: 0.9009 - val_loss: 4.6784 - val_accuracy: 0.5093\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3098 - accuracy: 0.9080 - val_loss: 5.0343 - val_accuracy: 0.5165\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3027 - accuracy: 0.9108 - val_loss: 4.7659 - val_accuracy: 0.5085\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3241 - accuracy: 0.9054 - val_loss: 4.2675 - val_accuracy: 0.4865\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3210 - accuracy: 0.9051 - val_loss: 4.8651 - val_accuracy: 0.4984\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3157 - accuracy: 0.9069 - val_loss: 5.2885 - val_accuracy: 0.5065\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3843 - accuracy: 0.8928 - val_loss: 4.8375 - val_accuracy: 0.5003\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3740 - accuracy: 0.8923 - val_loss: 4.8416 - val_accuracy: 0.5102\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4701 - accuracy: 0.8721 - val_loss: 4.6657 - val_accuracy: 0.4979\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3923 - accuracy: 0.8888 - val_loss: 4.6482 - val_accuracy: 0.5071\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3513 - accuracy: 0.8993 - val_loss: 4.6500 - val_accuracy: 0.5029\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3696 - accuracy: 0.8965 - val_loss: 4.7260 - val_accuracy: 0.5085\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3143 - accuracy: 0.9089 - val_loss: 5.2829 - val_accuracy: 0.4955\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3218 - accuracy: 0.9067 - val_loss: 5.1682 - val_accuracy: 0.5038\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3467 - accuracy: 0.9035 - val_loss: 5.1256 - val_accuracy: 0.4901\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3276 - accuracy: 0.9077 - val_loss: 5.1582 - val_accuracy: 0.4995\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3655 - accuracy: 0.9007 - val_loss: 5.3320 - val_accuracy: 0.4845\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3433 - accuracy: 0.9059 - val_loss: 5.7981 - val_accuracy: 0.5007\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3262 - accuracy: 0.9097 - val_loss: 5.9330 - val_accuracy: 0.5104\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3555 - accuracy: 0.9033 - val_loss: 5.2549 - val_accuracy: 0.4871\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4230 - accuracy: 0.8889 - val_loss: 5.6315 - val_accuracy: 0.4987\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3823 - accuracy: 0.8970 - val_loss: 5.2966 - val_accuracy: 0.4972\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f95319d3070>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Dense(2048, input_shape=(3072,), activation='relu'))\n",
        "model3.add(Dense(2048, activation='relu'))\n",
        "model3.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "model3.compile(loss='categorical_crossentropy', optimizer=sgd, metrics='accuracy')\n",
        "\n",
        "model3.fit(train_x, train_y, epochs=100, batch_size=32, validation_data=(test_x, test_y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7DwnwfKVD5nW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612ccc48-6df9-4919-c3f0-a0b4ada6d6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8350 - accuracy: 0.3387 - val_loss: 1.6682 - val_accuracy: 0.4004\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6611 - accuracy: 0.4066 - val_loss: 1.6223 - val_accuracy: 0.4191\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5789 - accuracy: 0.4329 - val_loss: 1.5039 - val_accuracy: 0.4637\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5146 - accuracy: 0.4553 - val_loss: 1.5069 - val_accuracy: 0.4691\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4651 - accuracy: 0.4748 - val_loss: 1.5128 - val_accuracy: 0.4654\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4292 - accuracy: 0.4867 - val_loss: 1.5094 - val_accuracy: 0.4617\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3918 - accuracy: 0.5009 - val_loss: 1.4531 - val_accuracy: 0.4766\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3515 - accuracy: 0.5164 - val_loss: 1.4849 - val_accuracy: 0.4753\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3104 - accuracy: 0.5297 - val_loss: 1.4643 - val_accuracy: 0.4826\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2803 - accuracy: 0.5427 - val_loss: 1.4307 - val_accuracy: 0.4964\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2457 - accuracy: 0.5523 - val_loss: 1.4734 - val_accuracy: 0.4934\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2222 - accuracy: 0.5655 - val_loss: 1.4118 - val_accuracy: 0.5085\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1886 - accuracy: 0.5728 - val_loss: 1.4198 - val_accuracy: 0.5106\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1606 - accuracy: 0.5851 - val_loss: 1.4530 - val_accuracy: 0.4910\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1298 - accuracy: 0.5932 - val_loss: 1.4958 - val_accuracy: 0.4865\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1060 - accuracy: 0.6034 - val_loss: 1.4553 - val_accuracy: 0.5023\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0678 - accuracy: 0.6148 - val_loss: 1.4505 - val_accuracy: 0.5149\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0399 - accuracy: 0.6260 - val_loss: 1.5132 - val_accuracy: 0.4982\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0148 - accuracy: 0.6346 - val_loss: 1.4713 - val_accuracy: 0.5181\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9781 - accuracy: 0.6492 - val_loss: 1.5190 - val_accuracy: 0.5107\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9471 - accuracy: 0.6602 - val_loss: 1.5808 - val_accuracy: 0.5228\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9242 - accuracy: 0.6697 - val_loss: 1.5571 - val_accuracy: 0.5099\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9025 - accuracy: 0.6744 - val_loss: 1.5566 - val_accuracy: 0.5132\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8768 - accuracy: 0.6846 - val_loss: 1.6035 - val_accuracy: 0.5122\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8489 - accuracy: 0.6960 - val_loss: 1.5982 - val_accuracy: 0.5230\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8189 - accuracy: 0.7046 - val_loss: 1.6431 - val_accuracy: 0.5045\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7997 - accuracy: 0.7142 - val_loss: 1.7520 - val_accuracy: 0.5087\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7647 - accuracy: 0.7254 - val_loss: 1.7111 - val_accuracy: 0.5221\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7445 - accuracy: 0.7336 - val_loss: 1.7381 - val_accuracy: 0.5066\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7281 - accuracy: 0.7391 - val_loss: 1.7896 - val_accuracy: 0.5134\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7212 - accuracy: 0.7420 - val_loss: 1.7696 - val_accuracy: 0.5114\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6852 - accuracy: 0.7557 - val_loss: 1.8345 - val_accuracy: 0.5134\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6741 - accuracy: 0.7603 - val_loss: 2.0800 - val_accuracy: 0.5082\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6605 - accuracy: 0.7657 - val_loss: 1.9841 - val_accuracy: 0.5030\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6185 - accuracy: 0.7798 - val_loss: 2.0545 - val_accuracy: 0.5147\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6295 - accuracy: 0.7774 - val_loss: 2.1301 - val_accuracy: 0.4965\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5948 - accuracy: 0.7878 - val_loss: 2.2437 - val_accuracy: 0.5094\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5948 - accuracy: 0.7907 - val_loss: 2.0734 - val_accuracy: 0.5083\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5657 - accuracy: 0.8002 - val_loss: 2.1591 - val_accuracy: 0.4849\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5629 - accuracy: 0.8020 - val_loss: 2.3248 - val_accuracy: 0.5190\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5613 - accuracy: 0.8007 - val_loss: 2.3002 - val_accuracy: 0.5125\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5542 - accuracy: 0.8027 - val_loss: 2.3165 - val_accuracy: 0.4738\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.5302 - accuracy: 0.8137 - val_loss: 2.3622 - val_accuracy: 0.5106\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5243 - accuracy: 0.8170 - val_loss: 2.4519 - val_accuracy: 0.5114\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.5117 - accuracy: 0.8211 - val_loss: 2.5443 - val_accuracy: 0.5150\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4842 - accuracy: 0.8315 - val_loss: 2.3907 - val_accuracy: 0.5026\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4841 - accuracy: 0.8327 - val_loss: 2.5955 - val_accuracy: 0.5100\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4574 - accuracy: 0.8398 - val_loss: 2.6563 - val_accuracy: 0.5050\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4605 - accuracy: 0.8402 - val_loss: 2.6384 - val_accuracy: 0.5019\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4794 - accuracy: 0.8351 - val_loss: 2.7060 - val_accuracy: 0.5025\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4551 - accuracy: 0.8426 - val_loss: 2.6919 - val_accuracy: 0.5133\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4763 - accuracy: 0.8381 - val_loss: 2.9189 - val_accuracy: 0.5018\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4450 - accuracy: 0.8457 - val_loss: 2.9899 - val_accuracy: 0.5087\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4814 - accuracy: 0.8378 - val_loss: 2.6079 - val_accuracy: 0.4870\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4146 - accuracy: 0.8554 - val_loss: 2.9656 - val_accuracy: 0.4986\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4292 - accuracy: 0.8516 - val_loss: 3.0562 - val_accuracy: 0.5113\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4345 - accuracy: 0.8525 - val_loss: 3.1415 - val_accuracy: 0.4925\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4202 - accuracy: 0.8578 - val_loss: 3.2977 - val_accuracy: 0.5088\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4205 - accuracy: 0.8592 - val_loss: 3.4011 - val_accuracy: 0.5143\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4053 - accuracy: 0.8626 - val_loss: 3.1490 - val_accuracy: 0.4999\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4603 - accuracy: 0.8490 - val_loss: 3.2609 - val_accuracy: 0.4990\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4300 - accuracy: 0.8568 - val_loss: 2.9353 - val_accuracy: 0.5008\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3881 - accuracy: 0.8681 - val_loss: 3.5140 - val_accuracy: 0.5143\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3999 - accuracy: 0.8661 - val_loss: 3.6617 - val_accuracy: 0.5063\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4176 - accuracy: 0.8636 - val_loss: 3.5841 - val_accuracy: 0.5012\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4273 - accuracy: 0.8590 - val_loss: 3.6014 - val_accuracy: 0.5075\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3961 - accuracy: 0.8689 - val_loss: 3.2380 - val_accuracy: 0.4963\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3761 - accuracy: 0.8768 - val_loss: 3.8286 - val_accuracy: 0.4995\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3955 - accuracy: 0.8705 - val_loss: 3.6752 - val_accuracy: 0.5071\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3909 - accuracy: 0.8719 - val_loss: 3.8943 - val_accuracy: 0.5001\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3893 - accuracy: 0.8729 - val_loss: 3.8834 - val_accuracy: 0.5095\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3498 - accuracy: 0.8869 - val_loss: 3.7820 - val_accuracy: 0.5076\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3819 - accuracy: 0.8785 - val_loss: 3.8282 - val_accuracy: 0.5162\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3532 - accuracy: 0.8853 - val_loss: 3.4575 - val_accuracy: 0.5046\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3512 - accuracy: 0.8856 - val_loss: 3.3562 - val_accuracy: 0.4822\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3750 - accuracy: 0.8796 - val_loss: 3.9895 - val_accuracy: 0.4968\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3807 - accuracy: 0.8778 - val_loss: 4.2837 - val_accuracy: 0.4983\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3854 - accuracy: 0.8773 - val_loss: 4.0175 - val_accuracy: 0.4869\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4086 - accuracy: 0.8729 - val_loss: 3.9870 - val_accuracy: 0.5019\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3621 - accuracy: 0.8834 - val_loss: 4.1889 - val_accuracy: 0.5052\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3844 - accuracy: 0.8795 - val_loss: 4.4751 - val_accuracy: 0.5045\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3980 - accuracy: 0.8759 - val_loss: 3.9588 - val_accuracy: 0.4826\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3990 - accuracy: 0.8760 - val_loss: 4.2783 - val_accuracy: 0.5061\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4253 - accuracy: 0.8693 - val_loss: 4.7980 - val_accuracy: 0.4962\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4099 - accuracy: 0.8738 - val_loss: 4.8130 - val_accuracy: 0.4903\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4170 - accuracy: 0.8720 - val_loss: 4.2763 - val_accuracy: 0.4993\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3601 - accuracy: 0.8878 - val_loss: 4.4501 - val_accuracy: 0.5048\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3750 - accuracy: 0.8841 - val_loss: 4.5237 - val_accuracy: 0.5003\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3881 - accuracy: 0.8794 - val_loss: 4.3359 - val_accuracy: 0.4982\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3469 - accuracy: 0.8919 - val_loss: 4.6963 - val_accuracy: 0.5101\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3693 - accuracy: 0.8871 - val_loss: 4.9913 - val_accuracy: 0.5088\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4008 - accuracy: 0.8804 - val_loss: 4.3938 - val_accuracy: 0.4810\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4573 - accuracy: 0.8663 - val_loss: 4.3936 - val_accuracy: 0.4638\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3930 - accuracy: 0.8829 - val_loss: 4.5421 - val_accuracy: 0.4954\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4005 - accuracy: 0.8805 - val_loss: 4.6723 - val_accuracy: 0.4978\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3494 - accuracy: 0.8927 - val_loss: 4.3737 - val_accuracy: 0.4937\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3912 - accuracy: 0.8823 - val_loss: 4.9710 - val_accuracy: 0.4825\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3963 - accuracy: 0.8841 - val_loss: 4.4699 - val_accuracy: 0.4888\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3940 - accuracy: 0.8835 - val_loss: 4.8567 - val_accuracy: 0.5041\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4163 - accuracy: 0.8772 - val_loss: 4.6156 - val_accuracy: 0.4895\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9531859940>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "model4 = Sequential()\n",
        "model4.add(Dense(2048, input_shape=(3072,), activation='relu'))\n",
        "model4.add(Dense(1024, activation='relu'))\n",
        "model4.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "model4.compile(loss='categorical_crossentropy', optimizer=sgd, metrics='accuracy')\n",
        "\n",
        "model4.fit(train_x, train_y, epochs=100, batch_size=32, validation_data=(test_x, test_y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 3 has the highest final validation accuracy, therefore we choose it as our final design"
      ],
      "metadata": {
        "id": "obooyTNrJqK2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HOCzVTJtNXM"
      },
      "source": [
        "----\n",
        "\n",
        "#### Question 4:\n",
        "\n",
        "Complete the following table with your final design (you may add more rows for the # neurons (layer1) etc. to detail how many neurons you have in each hidden layer). Likewise you may replace the lr, momentum etc rows with parameters more appropriate to the optimizer that you have chosen. (3 MARKS)\n",
        "\n",
        "\n",
        "| Hyperparameter       | What I used | Why?                  |\n",
        "|:---------------------|:------------|:----------------------|\n",
        "| Optimizer            |    SGD         |   We make it same with the previous 3.2 model                    |\n",
        "| # of hidden layers   |     2        |    We add one hidden layer and the accuracy becomes larger                   |\n",
        "| # neurons(layer1)    |     2048        |   We make it double to the previous 3.2 model                    |\n",
        "| Hid layer1 activation|       ReLU      |  We make it same with the previous 3.2 model                     |\n",
        "| # neurons(layer2)    |     2048        |   The accuracy is larger than that of 1024 in the experiment                    |\n",
        "| Hid layer2 activation|      ReLU       |   We make it same with the previous 3.2 model                    |\n",
        "| # of output neurons  |      10       |    There are 10 different classes in the dataset                   |\n",
        "| Output activation    |       Softmax      |    It can make the outputs represent the probabilitiy in each class, where the outputs are larger than 0 and are sum to 1                   |\n",
        "| lr                   |     0.01        |  We make it same with the previous 3.2 model                     |\n",
        "| momentum             |       0.9      |   We make it same with the previous 3.2 model                    |\n",
        "| decay                |      0       |    We make it same with the previous 3.2 model                   |\n",
        "| loss                 |     Categorical Cross Entropy        |          We make it same with the previous 3.2 model             |\n",
        "\n",
        "*FOR GRADER: _____ / 3 * <br>\n",
        "*CODE: ______ / 5 *<br>\n",
        "\n",
        "***TOTAL: ______ / 8***\n",
        "\n",
        "#### Question 5\n",
        "\n",
        "What is the final training and validation accuracy that you obtained after 150 epochs. Is there considerable improvement over Section 3.2? Are there still signs of underfitting or overfitting? Explain your answer (5 MARKS)\n",
        "\n",
        "***1. The final training accuracy is 0.8970 and the validation accuracy is 0.4972***\n",
        "\n",
        "***2. There is not considerable improvement over Section 3.2. There is only a little improvement***\n",
        "\n",
        "***3. There are still overfitting problem, because the training accuracy is much larger than the validation accuracy***\n",
        "\n",
        "\n",
        "\n",
        "*FOR GRADER: ______ / 5 *\n",
        "\n",
        "#### Question 6\n",
        "\n",
        "Write a short reflection on the practical difficulties of using a dense MLP to classsify images in the CIFAR-10 datasets. (3 MARKS)\n",
        "\n",
        "***1. The validation accuracy is low, which is only about 50%. ***\n",
        "\n",
        "***2. There are too many parameters for the MLP model and therefore will take a long time to train this model. ***\n",
        "\n",
        "***3. There is overfitting problem where the training accuracy is much larger than the validation accuracy. ***\n",
        "\n",
        "*FOR GRADER: _______ /3*\n",
        "\n",
        "----\n",
        "\n",
        "## 4. Creating a CNN for the MNIST Data Set\n",
        "\n",
        "In this section we will now create a convolutional neural network (CNN) to classify images in the MNIST dataset that we used in the previous lab. Let's go through each part to see how to do this.\n",
        "\n",
        "### 4.1 Loading the MNIST Dataset\n",
        "\n",
        "As always we will load the MNIST dataset, scale the inputs to between 0 and 1, and convert the Y labels to one-hot vectors. However unlike before we will not flatten the 28x28 image to a 784 element vector, since CNNs can inherently handle 2D data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JpLxxX2vtNXM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def load_mnist():\n",
        "    (train_x, train_y),(test_x, test_y) = mnist.load_data()\n",
        "    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n",
        "    test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n",
        "\n",
        "    train_x=train_x.astype('float32')\n",
        "    test_x = test_x.astype('float32')\n",
        "    \n",
        "    train_x /= 255.0\n",
        "    test_x /= 255.0\n",
        "        \n",
        "    train_y = to_categorical(train_y, 10)\n",
        "    test_y = to_categorical(test_y, 10)\n",
        "        \n",
        "    return (train_x, train_y), (test_x, test_y) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZEUB1eItNXM"
      },
      "source": [
        "### 4.2 Building the CNN\n",
        "\n",
        "We will now build the CNN. Unlike before we will create a function to produce the CNN. We will also look at how to save and load Keras models using \"checkpoints\", particularly \"ModelCheckpoint\" that saves the model each epoch.\n",
        "\n",
        "Let's begin by creating the model. We call os.path.exists to see if a model file exists, and call \"load_model\" if it does. Otherwise we create a new model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tSRX6IJVtNXM"
      },
      "outputs": [],
      "source": [
        "# load_model loads a model from a hd5 file.\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "MODEL_NAME = 'mnist-cnn.hd5'\n",
        "\n",
        "def buildmodel(model_name):\n",
        "    if os.path.exists(model_name):\n",
        "        model = load_model(model_name)                                                                                             \n",
        "    else:\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, kernel_size=(5,5),\n",
        "        activation='relu',\n",
        "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
        "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
        "        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
        "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "        model.add(Flatten()) # Question 9\n",
        "        model.add(Dense(1024, activation='relu'))\n",
        "        model.add(Dropout(0.1))\n",
        "        model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17_wQUs2tNXN"
      },
      "source": [
        "----\n",
        "\n",
        "#### Question 7\n",
        "\n",
        "The first layer in our CNN is a 2D convolution kernel, shown here:\n",
        "\n",
        "```\n",
        "        model.add(Conv2D(32, kernel_size=(5,5),\n",
        "        activation='relu',\n",
        "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
        "```\n",
        "\n",
        "Why is the input_shape set to (28, 28, 1)? What does this mean? What does \"padding = 'same'\" mean? (4 MARKS)\n",
        "\n",
        "***The input_shape is set to (28, 28, 1) because the images are of size 28 * 28 pixels and there is only one color channel (grayscale). Padding = 'same' means that the output image will have the same size as the input image after the convolution layer.***\n",
        "\n",
        "*FOR GRADER: ______ / 4*\n",
        "\n",
        "#### Question 8\n",
        "\n",
        "The second layer is the MaxPooling2D layer shown below:\n",
        "\n",
        "```\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
        "```\n",
        "\n",
        "What other types of pooling layers are available? What does 'strides = 2' mean? (3 MARKS)\n",
        "\n",
        "***Other types of pooling layers are Average Pooling layers, Global Max Pooling layers, Global Average Pooling layers. Strides = 2 means that the pooling filter will move for 2 pixels each time.***\n",
        "\n",
        "*FOR GRADER: _____ / 3*\n",
        "\n",
        "\n",
        "#### Question 9\n",
        "\n",
        "What does the \"Flatten\" layer here do? Why is it needed?\n",
        "\n",
        "```\n",
        "        model.add(Flatten()) # Question 9\n",
        "```\n",
        "\n",
        "***The Flatten layer will convert the 2D array into a 1D vector. It is needed because the following dense layer can only accept 1D arrays as input.***\n",
        "\n",
        "*FOR GRADER: ____ / 2*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "### 4.3 Training the CNN\n",
        "\n",
        "Let's now train the CNN. In this example we introduce the idea of a \"callback\", which is a routine that Keras calls at the end of each epoch. Specifically we look at two callbacks:\n",
        "\n",
        "    1. ModelCheckpoint: When called, Keras saves the model to the specified filename.\n",
        "    \n",
        "    2. EarlyStopping: When called, Keras checks if it should stop the training prematurely.\n",
        "    \n",
        "\n",
        "Let's look at the code to see how training is done, and how callbacks are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7l4YSUcrtNXN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n",
        "\n",
        "    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.7), \n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    savemodel = ModelCheckpoint(model_name)\n",
        "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
        "\n",
        "    print(\"Starting training.\")\n",
        "\n",
        "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
        "    validation_data=(test_x, test_y), shuffle=True,\n",
        "    epochs=epochs, \n",
        "    callbacks=[savemodel, stopmodel])\n",
        "\n",
        "    print(\"Done. Now evaluating.\")\n",
        "    loss, acc = model.evaluate(x=test_x, y=test_y)\n",
        "    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aOENccetNXN"
      },
      "source": [
        "Notice that there isn't very much that is unusual going on; we compile the model with our loss function and optimizer, then call fit, and finally evaluate to look at the final accuracy for the test set.  The only thing unusual is the \"callbacks\" parameter here in the fit function call\n",
        "\n",
        "```\n",
        "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
        "    validation_data=(test_x, test_y), shuffle=True,\n",
        "    epochs=epochs, \n",
        "    callbacks=[savemodel, stopmodel])\n",
        "```\n",
        "\n",
        "----\n",
        "\n",
        "#### Question 10.\n",
        "\n",
        "What does do the min_delta and patience parameters do in the EarlyStopping callback, as shown below? (2 MARKS)\n",
        "\n",
        "```\n",
        "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
        "```\n",
        "\n",
        "Answer: It will stop the training process if the validation loss does not decrease larger than 0.001(the value of min_delta) after conseuctive 10 (the value of patience) epochs.\n",
        "---\n",
        "\n",
        "### 4.4 Putting it together.\n",
        "\n",
        "Now let's run the code and see how it goes (Note: To save time we are training for only 5 epochs; we should train much longer to get much better results):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kscIeb4rtNXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad678dc-2178-4cd2-a362-d5209a0aafb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Starting training.\n",
            "Epoch 1/5\n",
            "1866/1875 [============================>.] - ETA: 0s - loss: 0.3484 - accuracy: 0.8910"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 18s 6ms/step - loss: 0.3474 - accuracy: 0.8913 - val_loss: 0.0893 - val_accuracy: 0.9731\n",
            "Epoch 2/5\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9776"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 11s 6ms/step - loss: 0.0716 - accuracy: 0.9776 - val_loss: 0.0491 - val_accuracy: 0.9842\n",
            "Epoch 3/5\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9848"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 11s 6ms/step - loss: 0.0480 - accuracy: 0.9848 - val_loss: 0.0376 - val_accuracy: 0.9876\n",
            "Epoch 4/5\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.0348 - accuracy: 0.9891"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 11s 6ms/step - loss: 0.0350 - accuracy: 0.9891 - val_loss: 0.0483 - val_accuracy: 0.9840\n",
            "Epoch 5/5\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.0277 - accuracy: 0.9911"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0277 - accuracy: 0.9911 - val_loss: 0.0378 - val_accuracy: 0.9869\n",
            "Done. Now evaluating.\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0378 - accuracy: 0.9869\n",
            "Test accuracy: 0.99, loss: 0.04\n"
          ]
        }
      ],
      "source": [
        "    (train_x, train_y),(test_x, test_y) = load_mnist()\n",
        "    model = buildmodel(MODEL_NAME)\n",
        "    train(model, train_x, train_y, 5, test_x, test_y, MODEL_NAME)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNTBsmTMtNXO"
      },
      "source": [
        "----\n",
        "\n",
        "#### Question 11.\n",
        "\n",
        "Compare the relative advantages and disadvantages of CNN vs. the Dense MLP that you build in sections 3.2 and 3.3. What makes CNNs better (or worse)? (3 MARKS)\n",
        "\n",
        "***We can see from the above experiment that CNN are better than Dense MLP. The reasons are: 1. The accuracy is much higher than MLP(99%). 2. The training speed of CNN is faster. 3. There is not overfitting problem. ***\n",
        "\n",
        "***The reasons make CNN better are given in the following. 1. In each layer, each outputs value depends only on a small number of outputs, which reduce the number of parameters and thus increase the training speed and avoid overfitting. 2. CNN filter can capture the similar features in different location of the images. ***\n",
        "\n",
        "*FOR TA: ______ / 3*\n",
        "\n",
        "## 5. Making a CNN for the CIFAR-10 Dataset\n",
        "\n",
        "Now comes the fun part: Using the example above for creating a CNN for the MNIST dataset, now create a CNN in the box below for the MNIST-10 dataset. At the end of each epoch save the model to a file called \"cifar.hd5\" (note: the .hd5 is added automatically for you).\n",
        "\n",
        "---\n",
        "\n",
        "#### Question 12.\n",
        "\n",
        "Summarize your design in the table below (the actual coding cell comes after this):\n",
        "\n",
        "| Hyperparameter       | What I used | Why?                  |\n",
        "|:---------------------|:------------|:----------------------|\n",
        "| Optimizer            |    SGD         |     We make it the same with previous CNN model                  |\n",
        "| Input shape          |    32 * 32 * 3         |  The height and the width is 32 and there are 3 color channels(RGB)                   |\n",
        "| First layer          |    Conv2D(32)         |  We make it the same with previous CNN model                     |\n",
        "| Second layer          |    MaxPooling2D         |  We make it the same with previous CNN model                     |\n",
        "| Third layer         |      Conv2D(64)       |     We make it the same with previous CNN model                  |\n",
        "| Fourth layer      |      Conv2D(128)       |  We make it the same with previous CNN model                     |\n",
        "| Fifth layer            |    Conv2D(64)         | We make it the same with previous CNN model                      |\n",
        "| Six layer          |     MaxPooling2D        |  We make it the same with previous CNN model                     |\n",
        "| Dnese layer          |     Dense(1024)        |  We make it the same with previous CNN model                     |\n",
        "| Dense layer          |     Dense(10)        |  There are 10 classes in the dataset                     |\n",
        "\n",
        "\n",
        "*FOR TA:*\n",
        "*Table: ________ / 3* <br>\n",
        "*Code: _________/ 7* <br>\n",
        "**TOTAL: _______ / 10** <br>\n",
        "\n",
        "---\n",
        "\n",
        "***TOTAL: _______ / 55***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_YM4IQVxtNXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "737f41ff-9919-44d6-996e-124617cfd6b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training.\n",
            "Epoch 1/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.7823 - accuracy: 0.3426"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 13s 7ms/step - loss: 1.7823 - accuracy: 0.3426 - val_loss: 1.4137 - val_accuracy: 0.4848\n",
            "Epoch 2/50\n",
            "1553/1563 [============================>.] - ETA: 0s - loss: 1.3410 - accuracy: 0.5146"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 1.3399 - accuracy: 0.5151 - val_loss: 1.2183 - val_accuracy: 0.5624\n",
            "Epoch 3/50\n",
            "1555/1563 [============================>.] - ETA: 0s - loss: 1.1492 - accuracy: 0.5926"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 10s 7ms/step - loss: 1.1487 - accuracy: 0.5928 - val_loss: 1.0823 - val_accuracy: 0.6130\n",
            "Epoch 4/50\n",
            "1557/1563 [============================>.] - ETA: 0s - loss: 1.0045 - accuracy: 0.6441"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 1.0041 - accuracy: 0.6442 - val_loss: 1.0475 - val_accuracy: 0.6373\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.8909 - accuracy: 0.6857"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.8909 - accuracy: 0.6857 - val_loss: 0.9479 - val_accuracy: 0.6698\n",
            "Epoch 6/50\n",
            "1553/1563 [============================>.] - ETA: 0s - loss: 0.7944 - accuracy: 0.7203"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.7944 - accuracy: 0.7203 - val_loss: 0.9128 - val_accuracy: 0.6843\n",
            "Epoch 7/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 0.7092 - accuracy: 0.7504"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.7090 - accuracy: 0.7505 - val_loss: 0.9197 - val_accuracy: 0.6936\n",
            "Epoch 8/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 0.6328 - accuracy: 0.7774"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.6327 - accuracy: 0.7774 - val_loss: 0.8718 - val_accuracy: 0.7118\n",
            "Epoch 9/50\n",
            "1558/1563 [============================>.] - ETA: 0s - loss: 0.5600 - accuracy: 0.8041"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 12s 7ms/step - loss: 0.5598 - accuracy: 0.8043 - val_loss: 0.8831 - val_accuracy: 0.7175\n",
            "Epoch 10/50\n",
            "1554/1563 [============================>.] - ETA: 0s - loss: 0.4887 - accuracy: 0.8274"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 12s 8ms/step - loss: 0.4883 - accuracy: 0.8275 - val_loss: 0.8910 - val_accuracy: 0.7205\n",
            "Epoch 11/50\n",
            "1556/1563 [============================>.] - ETA: 0s - loss: 0.4284 - accuracy: 0.8500"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 12s 7ms/step - loss: 0.4282 - accuracy: 0.8500 - val_loss: 0.8979 - val_accuracy: 0.7196\n",
            "Epoch 12/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 0.3676 - accuracy: 0.8708"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.3678 - accuracy: 0.8707 - val_loss: 0.9609 - val_accuracy: 0.7149\n",
            "Epoch 13/50\n",
            "1559/1563 [============================>.] - ETA: 0s - loss: 0.3150 - accuracy: 0.8876"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.3151 - accuracy: 0.8876 - val_loss: 1.1169 - val_accuracy: 0.6966\n",
            "Epoch 14/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 0.2795 - accuracy: 0.9016"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.2795 - accuracy: 0.9016 - val_loss: 1.1203 - val_accuracy: 0.6972\n",
            "Epoch 15/50\n",
            "1559/1563 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9121"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.2481 - accuracy: 0.9121 - val_loss: 1.1752 - val_accuracy: 0.7136\n",
            "Epoch 16/50\n",
            "1558/1563 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.9256"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.2139 - accuracy: 0.9256 - val_loss: 1.2784 - val_accuracy: 0.7064\n",
            "Epoch 17/50\n",
            "1559/1563 [============================>.] - ETA: 0s - loss: 0.1858 - accuracy: 0.9343"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.1860 - accuracy: 0.9342 - val_loss: 1.2445 - val_accuracy: 0.7041\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.1666 - accuracy: 0.9404"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1563/1563 [==============================] - 11s 7ms/step - loss: 0.1666 - accuracy: 0.9404 - val_loss: 1.3010 - val_accuracy: 0.7036\n",
            "Done. Now evaluating.\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 1.3010 - accuracy: 0.7036\n",
            "Test accuracy: 0.70, loss: 1.30\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Write your code for your CNN for the CIFAR-10 dataset here. \n",
        "\n",
        "Note: train_x, train_y, test_x, test_y were changed when we called \n",
        "load_mnist in the previous section. You will now need to call load_cifar10\n",
        "again.\n",
        "\n",
        "\"\"\"\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "CNN_MODEL_NAME = 'cifar.hd5'\n",
        "\n",
        "def cnn_model(model_name):\n",
        "    if os.path.exists(model_name):\n",
        "        model = load_model(model_name)                                                                                             \n",
        "    else:\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, kernel_size=(5,5),\n",
        "        activation='relu',\n",
        "        input_shape=(32, 32, 3), padding='same')) # Question 7\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
        "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
        "        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
        "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "        model.add(Flatten()) # Question 9\n",
        "        model.add(Dense(1024, activation='relu'))\n",
        "        model.add(Dropout(0.1))\n",
        "        model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n",
        "\n",
        "    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.7), \n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    savemodel = ModelCheckpoint(model_name)\n",
        "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
        "\n",
        "    print(\"Starting training.\")\n",
        "\n",
        "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
        "    validation_data=(test_x, test_y), shuffle=True,\n",
        "    epochs=epochs, \n",
        "    callbacks=[savemodel, stopmodel])\n",
        "\n",
        "    print(\"Done. Now evaluating.\")\n",
        "    loss, acc = model.evaluate(x=test_x, y=test_y)\n",
        "    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))\n",
        "\n",
        "\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
        "train_x = train_x.astype('float32')\n",
        "test_x = test_x.astype('float32')\n",
        "train_x /= 255.0\n",
        "test_x /= 255.0\n",
        "train_y = to_categorical(train_y,10)\n",
        "test_y = to_categorical(test_y, 10)\n",
        "\n",
        "model = cnn_model(CNN_MODEL_NAME)\n",
        "train(model, train_x, train_y, 50, test_x, test_y, MODEL_NAME)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}